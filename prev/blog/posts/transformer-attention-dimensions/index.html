<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Transformer Attention Matrix Dimensions - John&#39;s Machine Learning and Deep Learning Blog</title>
  <meta name="description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
  <meta name="author" content="John Chen"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "John\x27s Machine Learning and Deep Learning Blog",
    
    "url": "https:\/\/johnchennewyork-coder.github.io\/blog"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/johnchennewyork-coder.github.io\/blog"
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/johnchennewyork-coder.github.io\/blog",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/johnchennewyork-coder.github.io\/blog\/posts\/transformer-attention-dimensions\/",
          "name": "Transformer Attention Matrix Dimensions"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "John Chen"
  },
  "headline": "Transformer Attention Matrix Dimensions",
  "description" : "A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.",
  "inLanguage" : "en",
  "wordCount":  0 ,
  "datePublished" : "2025-12-08T17:02:21",
  "dateModified" : "2025-12-08T17:02:21",
  "image" : "https:\/\/johnchennewyork-coder.github.io\/blog\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/johnchennewyork-coder.github.io\/blog\/posts\/transformer-attention-dimensions\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/johnchennewyork-coder.github.io\/blog",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/johnchennewyork-coder.github.io\/blog\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Transformer Attention Matrix Dimensions" />
<meta property="og:description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
<meta property="og:image" content="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" />
<meta property="og:url" content="https://johnchennewyork-coder.github.io/blog/posts/transformer-attention-dimensions/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="John&#39;s Machine Learning and Deep Learning Blog" />

  <meta name="twitter:title" content="Transformer Attention Matrix Dimensions" />
  <meta name="twitter:description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
  <meta name="twitter:image" content="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary" />
  <link href='https://johnchennewyork-coder.github.io/blog/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.59.0" />
  <link rel="alternate" href="https://johnchennewyork-coder.github.io/blog/index.xml" type="application/rss+xml" title="John&#39;s Machine Learning and Deep Learning Blog"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/highlight.min.css" /><link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150888192-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://johnchennewyork-coder.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/blog">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/blog/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/blog/tags">Tags</a>
            </li>
          

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="John&#39;s Machine Learning and Deep Learning Blog" href="https://johnchennewyork-coder.github.io/blog">
            <img class="avatar-img" src="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" alt="John&#39;s Machine Learning and Deep Learning Blog" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  


  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="posts-heading">
              
                <h1>Transformer Attention Matrix Dimensions</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>Understanding matrix dimensions is crucial when implementing the Transformer architecture. This post provides a detailed walkthrough of how matrix dimensions change as data flows through a Transformer, from input tokens to the final output. We&rsquo;ll trace dimensions through embeddings, attention mechanisms, feed-forward networks, and output layers.</p>

<h2 id="notation">Notation</h2>

<p>Throughout this post, we&rsquo;ll use the following notation:</p>

<table style="width: 100%; margin: 20px 0;">
<thead>
<tr>
<th style="text-align: left; padding: 8px; border-bottom: 2px solid #ddd;">Symbol</th>
<th style="text-align: left; padding: 8px; border-bottom: 2px solid #ddd;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$B$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Batch size</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$n$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Sequence length (number of tokens)</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$V$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Vocabulary size</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_{\text{model}}$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Model dimension (embedding dimension), typically 512 in the original paper</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_k$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of keys and queries, typically $\frac{d_{\text{model}}}{h}$</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_v$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of values, typically $\frac{d_{\text{model}}}{h}$</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$h$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Number of attention heads</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_{\text{ff}}$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of feed-forward network, typically 2048 in the original paper</td>
</tr>
</tbody>
</table>

<p>For clarity, we&rsquo;ll often omit the batch dimension $B$ in our notation, but it should be understood that all tensors have an additional first dimension for batching.</p>

<h2 id="1-input-token-indices">1. Input: Token Indices</h2>

<p>The input to a Transformer is a sequence of token indices from the vocabulary.</p>

<p><strong>Matrix dimensions:</strong> $(B, n)$</p>

<p>Each element is an integer in the range $[0, V-1]$ representing a token ID. For example, with a batch size of 32 and sequence length of 100, we have a tensor of shape $(32, 100)$.</p>

<h2 id="2-word-embeddings">2. Word Embeddings</h2>

<p>The token indices are converted to dense vector representations through an embedding lookup.</p>

<p><strong>Embedding matrix:</strong> $(V, d_{\text{model}})$</p>

<p><strong>After embedding lookup:</strong> $(B, n, d_{\text{model}})$</p>

<p>The embedding layer contains a learnable matrix of size $(V, d_{\text{model}})$ where each row corresponds to a vocabulary token. The embedding lookup operation converts each token index to its corresponding $d_{\text{model}}$-dimensional vector.</p>

<p>For example, with $V = 50000$, $d_{\text{model}} = 512$, $B = 32$, and $n = 100$ (vocabulary size, model dimension, batch size, and sequence length respectively):</p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>Embedding matrix:</strong> $(50000, 512)$</li>
<li style="margin: 5px 0;"><strong>Embedded input:</strong> $(32, 100, 512)$</li>
</ul>

<h2 id="3-positional-encoding">3. Positional Encoding</h2>

<p>Since Transformers don&rsquo;t have recurrence or convolution, positional information must be added explicitly. Positional encodings are added (not concatenated) to the word embeddings.</p>

<p><strong>Positional encoding matrix:</strong> $(n, d_{\text{model}})$</p>

<p><strong>After adding positional encoding:</strong> $(B, n, d_{\text{model}})$</p>

<p>The positional encoding has the same dimensions as the embedded input. The addition is element-wise, broadcasting the $(n, d_{\text{model}})$ positional encoding across the batch dimension.</p>

<p>After this step, we have: <strong>$(B, n, d_{\text{model}})$</strong></p>

<h2 id="4-single-head-attention-mechanism">4. Single-Head Attention Mechanism</h2>

<p>Now we enter the attention mechanism. For a single attention head, we compute Query (Q), Key (K), and Value (V) matrices.</p>

<h3 id="41-query-key-value-projections">4.1. Query, Key, Value Projections</h3>

<p>Three linear transformations project the input into query, key, and value spaces.</p>

<p><strong>Input to projections:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrices:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$W_Q$:</strong> $(d_{\text{model}}, d_k)$</li>
<li style="margin: 5px 0;"><strong>$W_K$:</strong> $(d_{\text{model}}, d_k)$</li>
<li style="margin: 5px 0;"><strong>$W_V$:</strong> $(d_{\text{model}}, d_v)$</li>
</ul>

<p><strong>After projections:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$Q = XW_Q$:</strong> $(B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$K = XW_K$:</strong> $(B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$V = XW_V$:</strong> $(B, n, d_v)$</li>
</ul>

<p>In the original paper, $d_k = d_v = \frac{d_{\text{model}}}{h}$. For example, with model dimension $d_{\text{model}} = 512$ and number of heads $h = 8$, we have $d_k = d_v = 64$.</p>

<h3 id="42-attention-scores">4.2. Attention Scores</h3>

<p>Attention scores are computed as the dot product of queries and keys, scaled by $\sqrt{d_k}$.</p>

<p><strong>Computation:</strong> $S = \frac{QK^T}{\sqrt{d_k}}$</p>

<p><strong>Matrix dimensions:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$Q$:</strong> $(B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$K^T$:</strong> $(B, d_k, n)$ (transposed)</li>
<li style="margin: 5px 0;"><strong>$S$:</strong> $(B, n, n)$</li>
</ul>

<p>The attention score matrix $S$ has shape $(B, n, n)$ where $S_{ij}$ represents how much token $i$ should attend to token $j$.</p>

<h3 id="43-attention-weights">4.3. Attention Weights</h3>

<p>Apply softmax to the attention scores to get attention weights.</p>

<p><strong>Computation:</strong> $A = \text{softmax}(S)$</p>

<p><strong>Matrix dimensions:</strong> $(B, n, n)$</p>

<p>The softmax is applied along the last dimension (over keys), so each row sums to 1. The attention weight matrix $A$ has the same dimensions as the score matrix $S$.</p>

<h3 id="44-attention-output">4.4. Attention Output</h3>

<p>Multiply attention weights by values to get the output.</p>

<p><strong>Computation:</strong> $\text{Attention}(Q, K, V) = AV$</p>

<p><strong>Matrix dimensions:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$A$:</strong> $(B, n, n)$</li>
<li style="margin: 5px 0;"><strong>$V$:</strong> $(B, n, d_v)$</li>
<li style="margin: 5px 0;"><strong>Output:</strong> $(B, n, d_v)$</li>
</ul>

<p>For a single head, the attention output has dimensions $(B, n, d_v)$.</p>

<h2 id="5-multi-head-attention">5. Multi-Head Attention</h2>

<p>Multi-head attention runs $h$ parallel attention mechanisms, then concatenates and projects the results.</p>

<h3 id="51-multiple-heads">5.1. Multiple Heads</h3>

<p>Each head independently computes attention:</p>

<p><strong>Per-head output:</strong> $(B, n, d_v)$</p>

<p><strong>All heads (stacked):</strong> $(B, h, n, d_v)$</p>

<p>Or equivalently, if we concatenate along the feature dimension: <strong>$(B, n, h \cdot d_v)$</strong></p>

<h3 id="52-concatenation-and-output-projection">5.2. Concatenation and Output Projection</h3>

<p>The outputs from all heads are concatenated and projected through a linear layer.</p>

<p><strong>After concatenation:</strong> $(B, n, h \cdot d_v)$</p>

<p>Since $d_v = \frac{d_{\text{model}}}{h}$, we have $h \cdot d_v = d_{\text{model}}$, so: <strong>$(B, n, d_{\text{model}})$</strong></p>

<p><strong>Output projection weight:</strong> $W_O$: $(d_{\text{model}}, d_{\text{model}})$</p>

<p><strong>Final multi-head attention output:</strong> $(B, n, d_{\text{model}})$</p>

<p>The output projection is optional but commonly used. The final output maintains the same dimensions as the input to the attention mechanism.</p>

<h2 id="6-residual-connection-and-layer-normalization">6. Residual Connection and Layer Normalization</h2>

<p>After multi-head attention, we add a residual connection and apply layer normalization.</p>

<p><strong>Attention output:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Residual (original input):</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After residual addition:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After layer normalization:</strong> $(B, n, d_{\text{model}})$</p>

<p>Layer normalization operates along the feature dimension and doesn&rsquo;t change tensor shape.</p>

<h2 id="7-feed-forward-network">7. Feed-Forward Network</h2>

<p>The feed-forward network consists of two linear transformations with a ReLU activation in between.</p>

<h3 id="71-first-linear-layer">7.1. First Linear Layer</h3>

<p><strong>Input:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrix:</strong> $W_1$: $(d_{\text{model}}, d_{\text{ff}})$</p>

<p><strong>After first linear layer:</strong> $(B, n, d_{\text{ff}})$</p>

<p>In the original paper, $d_{\text{ff}} = 2048$, which is 4 times $d_{\text{model}} = 512$.</p>

<h3 id="72-relu-activation">7.2. ReLU Activation</h3>

<p><strong>After ReLU:</strong> $(B, n, d_{\text{ff}})$</p>

<p>The ReLU activation is element-wise and doesn&rsquo;t change dimensions.</p>

<h3 id="73-second-linear-layer">7.3. Second Linear Layer</h3>

<p><strong>Weight matrix:</strong> $W_2$: $(d_{\text{ff}}, d_{\text{model}})$</p>

<p><strong>After second linear layer:</strong> $(B, n, d_{\text{model}})$</p>

<p>The FFN expands to $$d_{\text{ff}}$$ dimensions and then projects back to $$d_{\text{model}}$$ dimensions.</p>

<h2 id="8-second-residual-and-layer-normalization">8. Second Residual and Layer Normalization</h2>

<p>After the FFN, we apply another residual connection and layer normalization.</p>

<p><strong>FFN output:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Residual (input to FFN):</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After residual addition:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After layer normalization:</strong> $(B, n, d_{\text{model}})$</p>

<h2 id="9-transformer-encoder-block-summary">9. Transformer Encoder Block Summary</h2>

<p>A complete Transformer encoder block maintains $(B, n, d_{\text{model}})$ dimensions throughout:</p>

<ol>
<li>Input: $(B, n, d_{\text{model}})$</li>
<li>Multi-head attention: $(B, n, d_{\text{model}})$</li>
<li>Residual + LayerNorm: $(B, n, d_{\text{model}})$</li>
<li>FFN: $(B, n, d_{\text{model}})$</li>
<li>Residual + LayerNorm: $(B, n, d_{\text{model}})$</li>
<li>Output: $(B, n, d_{\text{model}})$</li>
</ol>

<h2 id="10-output-layer-for-language-modeling">10. Output Layer (for Language Modeling)</h2>

<p>For language modeling tasks, the final output is projected to vocabulary size and passed through softmax to get token probabilities.</p>

<h3 id="101-output-projection">10.1. Output Projection</h3>

<p><strong>Input:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrix:</strong> $W_{\text{out}}$: $(d_{\text{model}}, V)$</p>

<p><strong>After projection:</strong> $(B, n, V)$</p>

<p>This projects each position&rsquo;s representation to vocabulary logits.</p>

<h3 id="102-softmax">10.2. Softmax</h3>

<p><strong>After softmax:</strong> $(B, n, V)$</p>

<p>The softmax is applied along the vocabulary dimension, giving probability distributions over the vocabulary for each position.</p>

<h2 id="11-decoder-attention-dimensions">11. Decoder Attention Dimensions</h2>

<p>The decoder has two types of attention: masked self-attention and encoder-decoder attention.</p>

<h3 id="111-masked-self-attention">11.1. Masked Self-Attention</h3>

<p>The decoder&rsquo;s self-attention uses a causal mask to prevent attending to future positions.</p>

<p><strong>Decoder input:</strong> $(B, m, d_{\text{model}})$ where $m$ is the target sequence length</p>

<p><strong>Attention scores:</strong> $(B, m, m)$ with mask applied</p>

<p><strong>Attention output:</strong> $(B, m, d_{\text{model}})$</p>

<p>The dimensions follow the same pattern as encoder self-attention, but with sequence length $$m$$ instead of $$n$$.</p>

<h3 id="112-encoder-decoder-attention">11.2. Encoder-Decoder Attention</h3>

<p>In encoder-decoder attention, queries come from the decoder while keys and values come from the encoder.</p>

<p><strong>Queries (from decoder):</strong> $(B, m, d_k)$</p>

<p><strong>Keys (from encoder):</strong> $(B, n, d_k)$</p>

<p><strong>Values (from encoder):</strong> $(B, n, d_v)$</p>

<p><strong>Attention scores:</strong> $QK^T = (B, m, d_k) \times (B, d_k, n) = (B, m, n)$</p>

<p><strong>Attention output:</strong> $(B, m, d_v)$</p>

<p>Note that the attention score matrix is $(B, m, n)$, not $(B, m, m)$ or $(B, n, n)$, because we&rsquo;re attending from decoder positions to encoder positions.</p>

<h2 id="12-complete-dimension-flow-example">12. Complete Dimension Flow Example</h2>

<p>Let&rsquo;s trace through a concrete example with the original Transformer paper parameters:</p>

<ul>
<li>$B = 32$ (batch size)</li>
<li>$n = 100$ (source sequence length)</li>
<li>$V = 50000$ (vocabulary size)</li>
<li>$d_{\text{model}} = 512$</li>
<li>$h = 8$ (number of heads)</li>
<li>$d_k = d_v = 64$ ($512/8$)</li>
<li>$d_{\text{ff}} = 2048$</li>
</ul>

<p><strong>Dimension flow:</strong></p>

<ol>
<li>Input tokens: $(32, 100)$</li>
<li>Word embeddings: $(32, 100, 512)$</li>
<li>+ Positional encoding: $(32, 100, 512)$</li>
<li>Multi-head attention:
   <ul>
   <li>Q, K, V projections: $(32, 100, 64)$ each</li>
   <li>Attention scores: $(32, 100, 100)$</li>
   <li>Attention weights: $(32, 100, 100)$</li>
   <li>Per-head output: $(32, 100, 64)$</li>
   <li>Concatenated: $(32, 100, 512)$</li>
   <li>Output projection: $(32, 100, 512)$</li>
   </ul>
</li>
<li>Residual + LayerNorm: $(32, 100, 512)$</li>
<li>Feed-forward network:
   <ul>
   <li>First linear: $(32, 100, 2048)$</li>
   <li>ReLU: $(32, 100, 2048)$</li>
   <li>Second linear: $(32, 100, 512)$</li>
   </ul>
</li>
<li>Residual + LayerNorm: $(32, 100, 512)$</li>
<li>Output projection: $(32, 100, 50000)$</li>
<li>Softmax: $(32, 100, 50000)$</li>
</ol>

<h2 id="13-key-insights">13. Key Insights</h2>

<ul>
<li><strong>Dimension preservation:</strong> The main data flow maintains $(B, n, d_{\text{model}})$ dimensions through most of the network, with temporary expansions in attention (to $(B, n, n)$ for scores) and FFN (to $(B, n, d_{\text{ff}})$).</li>

<li><strong>Attention complexity:</strong> The attention score matrix is quadratic in sequence length $(B, n, n)$, which is why Transformers have $O(n^2)$ complexity.</li>

<li><strong>Multi-head dimension math:</strong> With $h$ heads, each head uses $d_k = d_v = \frac{d_{\text{model}}}{h}$ dimensions. After concatenation, we get back to $d_{\text{model}}$ dimensions: $h \times \frac{d_{\text{model}}}{h} = d_{\text{model}}$.</li>

<li><strong>FFN expansion:</strong> The feed-forward network expands to $d_{\text{ff}}$ (typically $4 \times d_{\text{model}}$) before projecting back, allowing for richer representations.</li>
</ul>

<h2 id="14-conclusion">14. Conclusion</h2>

<p>Understanding matrix dimensions is essential for debugging Transformer implementations and ensuring correct tensor operations. The key pattern is that most operations preserve the sequence and model dimensions $(B, n, d_{\text{model}})$, with specific expansions for attention computations and feed-forward transformations. Keeping track of these dimensions at each step helps prevent shape mismatches and provides insight into the computational flow of the architecture.</p>

        
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;text=Transformer%20Attention%20Matrix%20Dimensions&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;description=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://johnchennewyork-coder.github.io/blog/posts/transformers_explained/" data-toggle="tooltip" data-placement="top" title="Transformers_explained">&larr; Previous Post</a>
            </li>
          
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              John Chen
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://johnchennewyork-coder.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.59.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://johnchennewyork-coder.github.io/blog/js/main.js"></script>
<script src="https://johnchennewyork-coder.github.io/blog/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body, {
  delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\[", right: "\\]", display: true},
    {left: "\\(", right: "\\)", display: false}
  ],
  throwOnError: false
}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://johnchennewyork-coder.github.io/blog/js/load-photoswipe.js"></script>





    
  </body>
</html>

