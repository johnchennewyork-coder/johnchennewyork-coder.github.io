<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Transformer Attention Matrix Dimensions - John&#39;s Machine Learning and Deep Learning Blog</title>
  <meta name="description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
  <meta name="author" content="John Chen"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "John\x27s Machine Learning and Deep Learning Blog",
    
    "url": "https:\/\/johnchennewyork-coder.github.io\/blog"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/johnchennewyork-coder.github.io\/blog"
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/johnchennewyork-coder.github.io\/blog",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/johnchennewyork-coder.github.io\/blog\/posts\/transformer-attention-dimensions\/",
          "name": "Transformer Attention Matrix Dimensions"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "John Chen"
  },
  "headline": "Transformer Attention Matrix Dimensions",
  "description" : "A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.",
  "inLanguage" : "en",
  "wordCount":  0 ,
  "datePublished" : "2025-12-08T17:02:21",
  "dateModified" : "2025-12-08T17:02:21",
  "image" : "https:\/\/johnchennewyork-coder.github.io\/blog\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/johnchennewyork-coder.github.io\/blog\/posts\/transformer-attention-dimensions\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/johnchennewyork-coder.github.io\/blog",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/johnchennewyork-coder.github.io\/blog\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Transformer Attention Matrix Dimensions" />
<meta property="og:description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
<meta property="og:image" content="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" />
<meta property="og:url" content="https://johnchennewyork-coder.github.io/blog/posts/transformer-attention-dimensions/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="John&#39;s Machine Learning and Deep Learning Blog" />

  <meta name="twitter:title" content="Transformer Attention Matrix Dimensions" />
  <meta name="twitter:description" content="A detailed walkthrough of matrix dimensions in the Transformer architecture, from input tokens through embeddings, attention mechanisms, feed-forward networks, and output layers. Understanding these dimensions is crucial for implementing Transformers correctly.">
  <meta name="twitter:image" content="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary" />
  <link href='https://johnchennewyork-coder.github.io/blog/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.59.0" />
  <link rel="alternate" href="https://johnchennewyork-coder.github.io/blog/index.xml" type="application/rss+xml" title="John&#39;s Machine Learning and Deep Learning Blog"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/highlight.min.css" /><link rel="stylesheet" href="https://johnchennewyork-coder.github.io/blog/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150888192-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://johnchennewyork-coder.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/blog">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/blog/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/blog/tags">Tags</a>
            </li>
          

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="John&#39;s Machine Learning and Deep Learning Blog" href="https://johnchennewyork-coder.github.io/blog">
            <img class="avatar-img" src="https://johnchennewyork-coder.github.io/blog/img/avatar-icon.png" alt="John&#39;s Machine Learning and Deep Learning Blog" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  


  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="posts-heading">
              
                <h1>Transformer Attention Matrix Dimensions</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>Understanding matrix dimensions is crucial when implementing the Transformer architecture. This post provides a detailed walkthrough of how matrix dimensions change as data flows through a Transformer, from input tokens to the final output. We&rsquo;ll trace dimensions through embeddings, attention mechanisms, feed-forward networks, and output layers.</p>

        <p><strong>Note:</strong> After embedding, the input to the Transformer is typically $(n, d_{\text{model}})$ for a single sequence (sequence length $n$ by model dimension $d_{\text{model}}$), or $(B, n, d_{\text{model}})$ when processing batches of size $B$. Throughout this post, we include the batch dimension $B$ for completeness, but the core pattern is $(n, d_{\text{model}})$ for a single sequence.</p>

<h2 id="notation">Notation</h2>

<p>Throughout this post, we&rsquo;ll use the following notation:</p>

<table style="width: 100%; margin: 20px 0;">
<thead>
<tr>
<th style="text-align: left; padding: 8px; border-bottom: 2px solid #ddd;">Symbol</th>
<th style="text-align: left; padding: 8px; border-bottom: 2px solid #ddd;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$B$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Batch size</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$n$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Sequence length (number of tokens)</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$V$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Vocabulary size</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_{\text{model}}$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Model dimension (embedding dimension), typically 512 in the original paper</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_k$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of keys and queries, typically $\frac{d_{\text{model}}}{h}$</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_v$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of values, typically $\frac{d_{\text{model}}}{h}$</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$h$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Number of attention heads</td>
</tr>
<tr>
<td style="padding: 8px; border-bottom: 1px solid #eee;">$d_{\text{ff}}$</td>
<td style="padding: 8px; border-bottom: 1px solid #eee;">Dimension of feed-forward network, typically 2048 in the original paper</td>
</tr>
</tbody>
</table>

<p>For clarity, we&rsquo;ll often omit the batch dimension $B$ in our notation, but it should be understood that all tensors have an additional first dimension for batching.</p>

<h2 id="1-input-token-indices">1. Input: Token Indices</h2>

<p>The input to a Transformer is a sequence of token indices from the vocabulary.</p>

<p><strong>Matrix dimensions:</strong> $(B, n)$</p>

<p>Each element is an integer in the range $[0, V-1]$ representing a token ID. For example, with a batch size of 32 and sequence length of 100, we have a tensor of shape $(32, 100)$.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="400" height="200" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .matrix-label { font-family: 'Times New Roman', serif; font-size: 14px; font-weight: bold; }
      .dim-label { font-family: 'Times New Roman', serif; font-size: 12px; fill: #666; }
      .matrix-cell { fill: #e3f2fd; stroke: #1976d2; stroke-width: 1.5; }
    </style>
  </defs>
  <!-- Matrix representation -->
  <rect x="100" y="50" width="200" height="100" class="matrix-cell" rx="5"/>
  <text x="200" y="30" text-anchor="middle" class="matrix-label">Input Tokens</text>
  <text x="200" y="175" text-anchor="middle" class="dim-label">$(B, n)$</text>
  <!-- Grid lines -->
  <line x1="150" y1="50" x2="150" y2="150" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <line x1="200" y1="50" x2="200" y2="150" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <line x1="250" y1="50" x2="250" y2="150" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <line x1="100" y1="75" x2="300" y2="75" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <line x1="100" y1="100" x2="300" y2="100" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <line x1="100" y1="125" x2="300" y2="125" stroke="#1976d2" stroke-width="0.5" opacity="0.5"/>
  <!-- Sample values -->
  <text x="125" y="95" text-anchor="middle" font-size="11" fill="#1976d2">42</text>
  <text x="175" y="95" text-anchor="middle" font-size="11" fill="#1976d2">17</text>
  <text x="225" y="95" text-anchor="middle" font-size="11" fill="#1976d2">...</text>
  <text x="275" y="95" text-anchor="middle" font-size="11" fill="#1976d2">n</text>
  <!-- Dimension labels -->
  <text x="50" y="100" text-anchor="middle" class="dim-label">$B$</text>
  <text x="200" y="165" text-anchor="middle" class="dim-label">$n$</text>
</svg>
</div>

<h2 id="2-word-embeddings">2. Word Embeddings</h2>

<p>The token indices are converted to dense vector representations through an embedding lookup.</p>

<p><strong>Embedding matrix:</strong> $(V, d_{\text{model}})$</p>

<p><strong>After embedding lookup:</strong> $(B, n, d_{\text{model}})$</p>

<p>The embedding layer contains a learnable matrix of size $(V, d_{\text{model}})$ where each row corresponds to a vocabulary token. The embedding lookup operation converts each token index to its corresponding $d_{\text{model}}$-dimensional vector. For a single sequence (without batching), this would be $(n, d_{\text{model}})$: $n$ tokens, each represented as a $d_{\text{model}}$-dimensional vector.</p>

<p>For example, with $V = 50000$, $d_{\text{model}} = 512$, $B = 32$, and $n = 100$ (vocabulary size, model dimension, batch size, and sequence length respectively):</p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>Embedding matrix:</strong> $(50000, 512)$</li>
<li style="margin: 5px 0;"><strong>Embedded input:</strong> $(32, 100, 512)$</li>
</ul>

<div style="text-align: center; margin: 20px 0;">
<svg width="600" height="300" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .embedding-matrix { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .input-tokens { fill: #e3f2fd; stroke: #1976d2; stroke-width: 2; }
      .output-embedding { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 13px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 11px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Embedding Matrix -->
  <rect x="20" y="80" width="120" height="180" class="embedding-matrix" rx="3"/>
  <text x="80" y="70" text-anchor="middle" class="label-text">Embedding Matrix</text>
  <text x="80" y="275" text-anchor="middle" class="dim-text">$(V, d_{\text{model}})$</text>
  <line x1="50" y1="80" x2="50" y2="260" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="80" y1="80" x2="80" y2="260" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="110" y1="80" x2="110" y2="260" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  
  <!-- Input Tokens -->
  <rect x="200" y="120" width="100" height="60" class="input-tokens" rx="3"/>
  <text x="250" y="110" text-anchor="middle" class="label-text">Input Tokens</text>
  <text x="250" y="195" text-anchor="middle" class="dim-text">$(B, n)$</text>
  
  <!-- Arrow -->
  <path d="M 140 150 L 200 150" class="arrow"/>
  <text x="170" y="145" text-anchor="middle" font-size="10" fill="#666">lookup</text>
  
  <!-- Output Embeddings -->
  <rect x="360" y="60" width="200" height="180" class="output-embedding" rx="3"/>
  <text x="460" y="50" text-anchor="middle" class="label-text">Embedded Input</text>
  <text x="460" y="255" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  <line x1="400" y1="60" x2="400" y2="240" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="440" y1="60" x2="440" y2="240" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="480" y1="60" x2="480" y2="240" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="520" y1="60" x2="520" y2="240" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="100" x2="560" y2="100" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="140" x2="560" y2="140" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="180" x2="560" y2="180" stroke="#388e3c" stroke-width="0.5" opacity="0.3"/>
  
  <!-- Arrow from input to output -->
  <path d="M 300 150 L 360 150" class="arrow"/>
</svg>
</div>

<h2 id="3-positional-encoding">3. Positional Encoding</h2>

<p>Since Transformers don&rsquo;t have recurrence or convolution, positional information must be added explicitly. Positional encodings are added (not concatenated) to the word embeddings.</p>

<p><strong>Positional encoding matrix:</strong> $(n, d_{\text{model}})$</p>

<p><strong>After adding positional encoding:</strong> $(B, n, d_{\text{model}})$</p>

<p>The positional encoding has the same dimensions as the embedded input. The addition is element-wise, broadcasting the $(n, d_{\text{model}})$ positional encoding across the batch dimension.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="550" height="250" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .embedding { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .pos-encoding { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .result { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .plus { font-size: 24px; fill: #666; font-weight: bold; }
      .label-text { font-family: 'Times New Roman', serif; font-size: 13px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 11px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Word Embeddings -->
  <rect x="20" y="60" width="180" height="120" class="embedding" rx="3"/>
  <text x="110" y="50" text-anchor="middle" class="label-text">Word Embeddings</text>
  <text x="110" y="195" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Plus sign -->
  <text x="220" y="130" text-anchor="middle" class="plus">+</text>
  
  <!-- Positional Encoding -->
  <rect x="250" y="60" width="180" height="120" class="pos-encoding" rx="3"/>
  <text x="340" y="50" text-anchor="middle" class="label-text">Positional Encoding</text>
  <text x="340" y="195" text-anchor="middle" class="dim-text">$(n, d_{\text{model}})$</text>
  
  <!-- Arrow -->
  <path d="M 430 120 L 470 120" class="arrow"/>
  
  <!-- Result -->
  <rect x="480" y="60" width="180" height="120" class="result" rx="3"/>
  <text x="570" y="50" text-anchor="middle" class="label-text">Result</text>
  <text x="570" y="195" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
</svg>
</div>

<p>After this step, we have: <strong>$(B, n, d_{\text{model}})$</strong></p>

<h2 id="4-single-head-attention-mechanism">4. Single-Head Attention Mechanism</h2>

<p>Now we enter the attention mechanism. For a single attention head, we compute Query (Q), Key (K), and Value (V) matrices.</p>

<h3 id="41-query-key-value-projections">4.1. Query, Key, Value Projections</h3>

<p>Three linear transformations project the input into query, key, and value spaces.</p>

<p><strong>Input to projections:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrices:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$W_Q$:</strong> $(d_{\text{model}}, d_k)$</li>
<li style="margin: 5px 0;"><strong>$W_K$:</strong> $(d_{\text{model}}, d_k)$</li>
<li style="margin: 5px 0;"><strong>$W_V$:</strong> $(d_{\text{model}}, d_v)$</li>
</ul>

<p><strong>After projections:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$Q = XW_Q$:</strong> $(B, n, d_{\text{model}}) \times (d_{\text{model}}, d_k) = (B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$K = XW_K$:</strong> $(B, n, d_{\text{model}}) \times (d_{\text{model}}, d_k) = (B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$V = XW_V$:</strong> $(B, n, d_{\text{model}}) \times (d_{\text{model}}, d_v) = (B, n, d_v)$</li>
</ul>

<p>In the original paper, $d_k = d_v = \frac{d_{\text{model}}}{h}$. For example, with model dimension $d_{\text{model}} = 512$ and number of heads $h = 8$, we have $d_k = d_v = 64$.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="650" height="280" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .input { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
      .weight { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .output { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Input X -->
  <rect x="20" y="100" width="140" height="100" class="input" rx="3"/>
  <text x="90" y="90" text-anchor="middle" class="label-text">Input $X$</text>
  <text x="90" y="215" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Q projection -->
  <rect x="200" y="20" width="100" height="60" class="weight" rx="3"/>
  <text x="250" y="10" text-anchor="middle" class="label-text">$W_Q$</text>
  <text x="250" y="90" text-anchor="middle" class="dim-text">$(d_{\text{model}}, d_k)$</text>
  <path d="M 160 130 L 200 50" class="arrow"/>
  <rect x="340" y="20" width="100" height="60" class="output" rx="3"/>
  <text x="390" y="10" text-anchor="middle" class="label-text">$Q$</text>
  <text x="390" y="90" text-anchor="middle" class="dim-text">$(B, n, d_k)$</text>
  <path d="M 300 50 L 340 50" class="arrow"/>
  
  <!-- K projection -->
  <rect x="200" y="100" width="100" height="60" class="weight" rx="3"/>
  <text x="250" y="90" text-anchor="middle" class="label-text">$W_K$</text>
  <text x="250" y="170" text-anchor="middle" class="dim-text">$(d_{\text{model}}, d_k)$</text>
  <path d="M 160 150 L 200 130" class="arrow"/>
  <rect x="340" y="100" width="100" height="60" class="output" rx="3"/>
  <text x="390" y="90" text-anchor="middle" class="label-text">$K$</text>
  <text x="390" y="170" text-anchor="middle" class="dim-text">$(B, n, d_k)$</text>
  <path d="M 300 130 L 340 130" class="arrow"/>
  
  <!-- V projection -->
  <rect x="200" y="180" width="100" height="60" class="weight" rx="3"/>
  <text x="250" y="170" text-anchor="middle" class="label-text">$W_V$</text>
  <text x="250" y="250" text-anchor="middle" class="dim-text">$(d_{\text{model}}, d_v)$</text>
  <path d="M 160 150 L 200 210" class="arrow"/>
  <rect x="340" y="180" width="100" height="60" class="output" rx="3"/>
  <text x="390" y="170" text-anchor="middle" class="label-text">$V$</text>
  <text x="390" y="250" text-anchor="middle" class="dim-text">$(B, n, d_v)$</text>
  <path d="M 300 210 L 340 210" class="arrow"/>
</svg>
</div>

<h3 id="42-attention-scores">4.2. Attention Scores</h3>

<p>Attention scores are computed as the dot product of queries and keys, scaled by $\sqrt{d_k}$.</p>

<p><strong>Computation:</strong> $S = \frac{QK^T}{\sqrt{d_k}}$</p>

<p><strong>Matrix dimensions:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$Q$:</strong> $(B, n, d_k)$</li>
<li style="margin: 5px 0;"><strong>$K$:</strong> $(B, n, d_k)$ → <strong>$K^T$:</strong> $(B, d_k, n)$ (transposing last two dimensions)</li>
<li style="margin: 5px 0;"><strong>$S = QK^T$:</strong> $(B, n, d_k) \times (B, d_k, n) = (B, n, n)$</li>
</ul>

<p>The attention score matrix $S$ has shape $(B, n, n)$ where $S_{ij}$ represents how much token $i$ should attend to token $j$.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="600" height="280" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .matrix { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .square-matrix { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
      .op-text { font-size: 16px; fill: #666; font-weight: bold; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Q matrix -->
  <rect x="20" y="80" width="120" height="80" class="matrix" rx="3"/>
  <text x="80" y="70" text-anchor="middle" class="label-text">$Q$</text>
  <text x="80" y="175" text-anchor="middle" class="dim-text">$(B, n, d_k)$</text>
  
  <!-- K^T matrix -->
  <rect x="180" y="80" width="120" height="80" class="matrix" rx="3"/>
  <text x="240" y="70" text-anchor="middle" class="label-text">$K^T$</text>
  <text x="240" y="175" text-anchor="middle" class="dim-text">$(B, d_k, n)$</text>
  
  <!-- Multiply sign -->
  <text x="320" y="125" text-anchor="middle" class="op-text">×</text>
  
  <!-- Result S -->
  <rect x="360" y="60" width="140" height="140" class="square-matrix" rx="3"/>
  <text x="430" y="50" text-anchor="middle" class="label-text">$S = QK^T$</text>
  <text x="430" y="215" text-anchor="middle" class="dim-text">$(B, n, n)$</text>
  <!-- Grid for attention scores -->
  <line x1="390" y1="60" x2="390" y2="200" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="420" y1="60" x2="420" y2="200" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="450" y1="60" x2="450" y2="200" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="100" x2="500" y2="100" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="130" x2="500" y2="130" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="160" x2="500" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  
  <!-- Arrows -->
  <path d="M 140 120 L 180 120" class="arrow"/>
  <path d="M 300 120 L 360 120" class="arrow"/>
</svg>
</div>

<h3 id="43-attention-weights">4.3. Attention Weights</h3>

<p>Apply softmax to the attention scores to get attention weights.</p>

<p><strong>Computation:</strong> $A = \text{softmax}(S)$</p>

<p><strong>Matrix dimensions:</strong> $(B, n, n)$</p>

<p>The softmax is applied along the last dimension (over keys), so each row sums to 1. The attention weight matrix $A$ has the same dimensions as the score matrix $S$.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .matrix { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
      .op-text { font-size: 14px; fill: #666; font-weight: bold; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- S matrix -->
  <rect x="20" y="60" width="140" height="140" class="matrix" rx="3"/>
  <text x="90" y="50" text-anchor="middle" class="label-text">$S$ (scores)</text>
  <text x="90" y="215" text-anchor="middle" class="dim-text">$(B, n, n)$</text>
  
  <!-- Softmax function -->
  <text x="200" y="130" text-anchor="middle" class="op-text">softmax</text>
  <path d="M 160 130 L 240 130" class="arrow"/>
  
  <!-- A matrix -->
  <rect x="280" y="60" width="140" height="140" class="matrix" rx="3"/>
  <text x="350" y="50" text-anchor="middle" class="label-text">$A$ (weights)</text>
  <text x="350" y="215" text-anchor="middle" class="dim-text">$(B, n, n)$</text>
  <!-- Highlight that rows sum to 1 -->
  <text x="350" y="235" text-anchor="middle" font-size="9" fill="#666" font-style="italic">Each row sums to 1</text>
</svg>
</div>

<h3 id="44-attention-output">4.4. Attention Output</h3>

<p>Multiply attention weights by values to get the output.</p>

<p><strong>Computation:</strong> $\text{Attention}(Q, K, V) = AV$</p>

<p><strong>Matrix dimensions:</strong></p>
<ul style="list-style-type: none; padding-left: 0;">
<li style="margin: 5px 0;"><strong>$A$:</strong> $(B, n, n)$</li>
<li style="margin: 5px 0;"><strong>$V$:</strong> $(B, n, d_v)$</li>
<li style="margin: 5px 0;"><strong>Output $= AV$:</strong> $(B, n, n) \times (B, n, d_v) = (B, n, d_v)$</li>
</ul>

<div style="text-align: center; margin: 20px 0;">
<svg width="600" height="280" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .matrix { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .output { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
      .op-text { font-size: 16px; fill: #666; font-weight: bold; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- A matrix -->
  <rect x="20" y="60" width="140" height="140" class="matrix" rx="3"/>
  <text x="90" y="50" text-anchor="middle" class="label-text">$A$</text>
  <text x="90" y="215" text-anchor="middle" class="dim-text">$(B, n, n)$</text>
  
  <!-- V matrix -->
  <rect x="200" y="80" width="120" height="100" class="matrix" rx="3"/>
  <text x="260" y="70" text-anchor="middle" class="label-text">$V$</text>
  <text x="260" y="195" text-anchor="middle" class="dim-text">$(B, n, d_v)$</text>
  
  <!-- Multiply sign -->
  <text x="350" y="130" text-anchor="middle" class="op-text">×</text>
  
  <!-- Result -->
  <rect x="380" y="80" width="120" height="100" class="output" rx="3"/>
  <text x="440" y="70" text-anchor="middle" class="label-text">Output</text>
  <text x="440" y="195" text-anchor="middle" class="dim-text">$(B, n, d_v)$</text>
  
  <!-- Arrows -->
  <path d="M 160 130 L 200 130" class="arrow"/>
  <path d="M 320 130 L 380 130" class="arrow"/>
</svg>
</div>

<p>For a single head, the attention output has dimensions $(B, n, d_v)$.</p>

<h2 id="5-multi-head-attention">5. Multi-Head Attention</h2>

<p>Multi-head attention runs $h$ parallel attention mechanisms, then concatenates and projects the results.</p>

<h3 id="51-multiple-heads">5.1. Multiple Heads</h3>

<p>Each head independently computes attention:</p>

<p><strong>Per-head output:</strong> $(B, n, d_v)$</p>

<p><strong>All heads (stacked):</strong> $(B, h, n, d_v)$</p>

<p>Or equivalently, if we concatenate along the feature dimension: <strong>$(B, n, h \cdot d_v)$</strong></p>

<div style="text-align: center; margin: 20px 0;">
<svg width="700" height="300" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .head-output { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .concatenated { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .final-output { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 11px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 9px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Multiple heads -->
  <rect x="20" y="40" width="80" height="60" class="head-output" rx="3"/>
  <text x="60" y="30" text-anchor="middle" class="label-text">Head 1</text>
  <text x="60" y="110" text-anchor="middle" class="dim-text">$(B, n, d_v)$</text>
  
  <rect x="120" y="40" width="80" height="60" class="head-output" rx="3"/>
  <text x="160" y="30" text-anchor="middle" class="label-text">Head 2</text>
  
  <text x="220" y="70" text-anchor="middle" font-size="14" fill="#666">...</text>
  
  <rect x="260" y="40" width="80" height="60" class="head-output" rx="3"/>
  <text x="300" y="30" text-anchor="middle" class="label-text">Head $h$</text>
  
  <!-- Concatenate arrow -->
  <path d="M 340 70 L 380 70" class="arrow"/>
  <text x="360" y="65" text-anchor="middle" font-size="10" fill="#666">concat</text>
  
  <!-- Concatenated -->
  <rect x="400" y="20" width="180" height="100" class="concatenated" rx="3"/>
  <text x="490" y="10" text-anchor="middle" class="label-text">Concatenated</text>
  <text x="490" y="135" text-anchor="middle" class="dim-text">$(B, n, h \cdot d_v)$</text>
  <!-- Show concatenation visually -->
  <line x1="440" y1="20" x2="440" y2="120" stroke="#f57c00" stroke-width="1" opacity="0.5"/>
  <line x1="480" y1="20" x2="480" y2="120" stroke="#f57c00" stroke-width="1" opacity="0.5"/>
  <line x1="520" y1="20" x2="520" y2="120" stroke="#f57c00" stroke-width="1" opacity="0.5"/>
  <line x1="560" y1="20" x2="560" y2="120" stroke="#f57c00" stroke-width="1" opacity="0.5"/>
  
  <!-- Output projection arrow -->
  <path d="M 580 70 L 620 70" class="arrow"/>
  
  <!-- Final output -->
  <rect x="640" y="40" width="120" height="60" class="final-output" rx="3"/>
  <text x="700" y="30" text-anchor="middle" class="label-text">Final Output</text>
  <text x="700" y="110" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
</svg>
</div>

<h3 id="52-concatenation-and-output-projection">5.2. Concatenation and Output Projection</h3>

<p>The outputs from all heads are concatenated and projected through a linear layer.</p>

<p><strong>After concatenation:</strong> $(B, n, h \cdot d_v)$</p>

<p>Since $d_v = \frac{d_{\text{model}}}{h}$, we have $h \cdot d_v = d_{\text{model}}$, so the concatenated output is: <strong>$(B, n, d_{\text{model}})$</strong></p>

<p><strong>Output projection weight:</strong> $W_O$: $(h \cdot d_v, d_{\text{model}})$ which equals $(d_{\text{model}}, d_{\text{model}})$ since $h \cdot d_v = d_{\text{model}}$</p>

<p><strong>Final multi-head attention output:</strong> $(B, n, d_{\text{model}})$</p>

<p>The output projection is optional but commonly used. The final output maintains the same dimensions as the input to the attention mechanism.</p>

<h2 id="6-residual-connection-and-layer-normalization">6. Residual Connection and Layer Normalization</h2>

<p>After multi-head attention, we add a residual connection and apply layer normalization.</p>

<p><strong>Attention output:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Residual (original input):</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After residual addition:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After layer normalization:</strong> $(B, n, d_{\text{model}})$</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .tensor { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .plus { font-size: 20px; fill: #666; font-weight: bold; }
      .label-text { font-family: 'Times New Roman', serif; font-size: 11px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 9px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Attention output -->
  <rect x="20" y="60" width="120" height="60" class="tensor" rx="3"/>
  <text x="80" y="50" text-anchor="middle" class="label-text">Attention Output</text>
  <text x="80" y="135" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Plus -->
  <text x="160" y="95" text-anchor="middle" class="plus">+</text>
  
  <!-- Original input -->
  <rect x="200" y="60" width="120" height="60" class="tensor" rx="3"/>
  <text x="260" y="50" text-anchor="middle" class="label-text">Original Input</text>
  <text x="260" y="135" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Arrow -->
  <path d="M 320 90 L 360 90" class="arrow"/>
  
  <!-- After addition -->
  <rect x="380" y="60" width="120" height="60" class="tensor" rx="3"/>
  <text x="440" y="50" text-anchor="middle" class="label-text">After Addition</text>
  <text x="440" y="135" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- LayerNorm arrow -->
  <path d="M 440 120 L 440 150" class="arrow"/>
  <text x="450" y="145" text-anchor="middle" font-size="9" fill="#666">LayerNorm</text>
  
  <!-- Final output -->
  <rect x="380" y="160" width="120" height="30" class="tensor" rx="3"/>
  <text x="440" y="205" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
</svg>
</div>

<p>Layer normalization operates along the feature dimension and doesn&rsquo;t change tensor shape.</p>

<h2 id="7-feed-forward-network">7. Feed-Forward Network</h2>

<p>The feed-forward network consists of two linear transformations with a ReLU activation in between.</p>

<h3 id="71-first-linear-layer">7.1. First Linear Layer</h3>

<p><strong>Input:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrix:</strong> $W_1$: $(d_{\text{model}}, d_{\text{ff}})$</p>

<p><strong>After first linear layer:</strong> $(B, n, d_{\text{model}}) \times (d_{\text{model}}, d_{\text{ff}}) = (B, n, d_{\text{ff}})$</p>

<p>In the original paper, $d_{\text{ff}} = 2048$, which is 4 times $d_{\text{model}} = 512$.</p>

<h3 id="72-relu-activation">7.2. ReLU Activation</h3>

<p><strong>After ReLU:</strong> $(B, n, d_{\text{ff}})$</p>

<p>The ReLU activation is element-wise and doesn&rsquo;t change dimensions.</p>

<h3 id="73-second-linear-layer">7.3. Second Linear Layer</h3>

<p><strong>Weight matrix:</strong> $W_2$: $(d_{\text{ff}}, d_{\text{model}})$</p>

<p><strong>After second linear layer:</strong> $(B, n, d_{\text{ff}}) \times (d_{\text{ff}}, d_{\text{model}}) = (B, n, d_{\text{model}})$</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="600" height="250" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .input { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
      .expanded { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .output { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
      .op-text { font-size: 12px; fill: #666; font-weight: bold; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Input -->
  <rect x="20" y="80" width="120" height="80" class="input" rx="3"/>
  <text x="80" y="70" text-anchor="middle" class="label-text">Input</text>
  <text x="80" y="175" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- W1 arrow -->
  <path d="M 140 120 L 180 120" class="arrow"/>
  <text x="160" y="115" text-anchor="middle" class="op-text">$W_1$</text>
  
  <!-- Expanded -->
  <rect x="200" y="40" width="140" height="160" class="expanded" rx="3"/>
  <text x="270" y="30" text-anchor="middle" class="label-text">Expanded</text>
  <text x="270" y="215" text-anchor="middle" class="dim-text">$(B, n, d_{\text{ff}})$</text>
  <text x="270" y="230" text-anchor="middle" font-size="9" fill="#666" font-style="italic">ReLU applied</text>
  
  <!-- W2 arrow -->
  <path d="M 340 120 L 380 120" class="arrow"/>
  <text x="360" y="115" text-anchor="middle" class="op-text">$W_2$</text>
  
  <!-- Output -->
  <rect x="400" y="80" width="120" height="80" class="output" rx="3"/>
  <text x="460" y="70" text-anchor="middle" class="label-text">Output</text>
  <text x="460" y="175" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
</svg>
</div>

<p>The FFN expands to $d_{\text{ff}}$ dimensions and then projects back to $d_{\text{model}}$ dimensions.</p>

<h2 id="8-second-residual-and-layer-normalization">8. Second Residual and Layer Normalization</h2>

<p>After the FFN, we apply another residual connection and layer normalization.</p>

<p><strong>FFN output:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Residual (input to FFN):</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After residual addition:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>After layer normalization:</strong> $(B, n, d_{\text{model}})$</p>

<h2 id="9-transformer-encoder-block-summary">9. Transformer Encoder Block Summary</h2>

<p>A complete Transformer encoder block maintains $(B, n, d_{\text{model}})$ dimensions throughout:</p>

<div style="text-align: center; margin: 30px 0;">
<svg width="400" height="500" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .box { fill: #f5f5f5; stroke: #666; stroke-width: 1.5; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .residual-arrow { stroke: #9c27b0; stroke-width: 2; fill: none; stroke-dasharray: 5,5; }
      .label-text { font-family: 'Times New Roman', serif; font-size: 11px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 9px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Input -->
  <rect x="100" y="20" width="200" height="40" class="box" rx="3"/>
  <text x="200" y="45" text-anchor="middle" class="label-text">Input</text>
  <text x="200" y="70" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Multi-head attention -->
  <path d="M 200 60 L 200 90" class="arrow"/>
  <rect x="80" y="90" width="240" height="50" class="box" rx="3"/>
  <text x="200" y="110" text-anchor="middle" class="label-text">Multi-head Attention</text>
  <text x="200" y="130" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Residual connection 1 -->
  <path d="M 100 115 L 100 140 L 80 140 L 80 20 L 100 20" class="residual-arrow"/>
  
  <!-- Residual + LayerNorm 1 -->
  <path d="M 200 140 L 200 170" class="arrow"/>
  <rect x="100" y="170" width="200" height="40" class="box" rx="3"/>
  <text x="200" y="195" text-anchor="middle" class="label-text">Residual + LayerNorm</text>
  <text x="200" y="220" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- FFN -->
  <path d="M 200 210 L 200 240" class="arrow"/>
  <rect x="80" y="240" width="240" height="50" class="box" rx="3"/>
  <text x="200" y="260" text-anchor="middle" class="label-text">Feed-Forward Network</text>
  <text x="200" y="280" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Residual connection 2 -->
  <path d="M 100 265 L 100 290 L 80 290 L 80 170 L 100 170" class="residual-arrow"/>
  
  <!-- Residual + LayerNorm 2 -->
  <path d="M 200 290 L 200 320" class="arrow"/>
  <rect x="100" y="320" width="200" height="40" class="box" rx="3"/>
  <text x="200" y="345" text-anchor="middle" class="label-text">Residual + LayerNorm</text>
  <text x="200" y="370" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Output -->
  <path d="M 200 360 L 200 390" class="arrow"/>
  <rect x="100" y="390" width="200" height="40" class="box" rx="3"/>
  <text x="200" y="415" text-anchor="middle" class="label-text">Output</text>
  <text x="200" y="440" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
</svg>
</div>

<ol>
<li>Input: $(B, n, d_{\text{model}})$</li>
<li>Multi-head attention: $(B, n, d_{\text{model}})$</li>
<li>Residual + LayerNorm: $(B, n, d_{\text{model}})$</li>
<li>FFN: $(B, n, d_{\text{model}})$</li>
<li>Residual + LayerNorm: $(B, n, d_{\text{model}})$</li>
<li>Output: $(B, n, d_{\text{model}})$</li>
</ol>

<h2 id="10-output-layer-for-language-modeling">10. Output Layer (for Language Modeling)</h2>

<p>For language modeling tasks, the final output is projected to vocabulary size and passed through softmax to get token probabilities.</p>

<h3 id="101-output-projection">10.1. Output Projection</h3>

<p><strong>Input:</strong> $(B, n, d_{\text{model}})$</p>

<p><strong>Weight matrix:</strong> $W_{\text{out}}$: $(d_{\text{model}}, V)$</p>

<p><strong>After projection:</strong> $(B, n, d_{\text{model}}) \times (d_{\text{model}}, V) = (B, n, V)$</p>

<p>This projects each position&rsquo;s representation to vocabulary logits.</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .input { fill: #e1bee7; stroke: #7b1fa2; stroke-width: 2; }
      .output { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Input -->
  <rect x="20" y="60" width="180" height="80" class="input" rx="3"/>
  <text x="110" y="50" text-anchor="middle" class="label-text">Input</text>
  <text x="110" y="155" text-anchor="middle" class="dim-text">$(B, n, d_{\text{model}})$</text>
  
  <!-- Arrow -->
  <path d="M 200 100 L 240 100" class="arrow"/>
  <text x="220" y="95" text-anchor="middle" font-size="10" fill="#666">$W_{\text{out}}$</text>
  
  <!-- Output -->
  <rect x="260" y="40" width="200" height="120" class="output" rx="3"/>
  <text x="360" y="30" text-anchor="middle" class="label-text">Output Logits</text>
  <text x="360" y="175" text-anchor="middle" class="dim-text">$(B, n, V)$</text>
  <!-- Show vocabulary dimension -->
  <line x1="280" y1="40" x2="280" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="320" y1="40" x2="320" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="360" y1="40" x2="360" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="400" y1="40" x2="400" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
  <line x1="440" y1="40" x2="440" y2="160" stroke="#f57c00" stroke-width="0.5" opacity="0.3"/>
</svg>
</div>

<h3 id="102-softmax">10.2. Softmax</h3>

<p><strong>After softmax:</strong> $(B, n, V)$</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .logits { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .probs { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 12px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 10px; fill: #666; }
      .op-text { font-size: 14px; fill: #666; font-weight: bold; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Logits -->
  <rect x="20" y="40" width="200" height="120" class="logits" rx="3"/>
  <text x="120" y="30" text-anchor="middle" class="label-text">Logits</text>
  <text x="120" y="175" text-anchor="middle" class="dim-text">$(B, n, V)$</text>
  
  <!-- Softmax -->
  <text x="260" y="100" text-anchor="middle" class="op-text">softmax</text>
  <path d="M 220 100 L 300 100" class="arrow"/>
  
  <!-- Probabilities -->
  <rect x="320" y="40" width="200" height="120" class="probs" rx="3"/>
  <text x="420" y="30" text-anchor="middle" class="label-text">Probabilities</text>
  <text x="420" y="175" text-anchor="middle" class="dim-text">$(B, n, V)$</text>
  <text x="420" y="190" text-anchor="middle" font-size="9" fill="#666" font-style="italic">Each row sums to 1</text>
</svg>
</div>

<p>The softmax is applied along the vocabulary dimension, giving probability distributions over the vocabulary for each position.</p>

<h2 id="11-decoder-attention-dimensions">11. Decoder Attention Dimensions</h2>

<p>The decoder has two types of attention: masked self-attention and encoder-decoder attention.</p>

<h3 id="111-masked-self-attention">11.1. Masked Self-Attention</h3>

<p>The decoder&rsquo;s self-attention uses a causal mask to prevent attending to future positions.</p>

<p><strong>Decoder input:</strong> $(B, m, d_{\text{model}})$ where $m$ is the target sequence length</p>

<p><strong>Attention scores:</strong> $(B, m, m)$ with mask applied</p>

<p><strong>Attention output:</strong> $(B, m, d_{\text{model}})$</p>

<p>The dimensions follow the same pattern as encoder self-attention, but with sequence length $m$ instead of $n$.</p>

<h3 id="112-encoder-decoder-attention">11.2. Encoder-Decoder Attention</h3>

<p>In encoder-decoder attention, queries come from the decoder while keys and values come from the encoder.</p>

<p><strong>Queries (from decoder):</strong> $(B, m, d_k)$</p>

<p><strong>Keys (from encoder):</strong> $(B, n, d_k)$</p>

<p><strong>Values (from encoder):</strong> $(B, n, d_v)$</p>

<p><strong>Attention scores:</strong> $QK^T = (B, m, d_k) \times (B, d_k, n) = (B, m, n)$</p>

<p><strong>Attention output:</strong> $(B, m, d_v)$</p>

<div style="text-align: center; margin: 20px 0;">
<svg width="600" height="280" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .decoder { fill: #e3f2fd; stroke: #1976d2; stroke-width: 2; }
      .encoder { fill: #fff3e0; stroke: #f57c00; stroke-width: 2; }
      .attention { fill: #e8f5e9; stroke: #388e3c; stroke-width: 2; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 11px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 9px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Decoder queries -->
  <rect x="20" y="80" width="120" height="80" class="decoder" rx="3"/>
  <text x="80" y="70" text-anchor="middle" class="label-text">Decoder $Q$</text>
  <text x="80" y="175" text-anchor="middle" class="dim-text">$(B, m, d_k)$</text>
  
  <!-- Encoder keys -->
  <rect x="180" y="80" width="120" height="80" class="encoder" rx="3"/>
  <text x="240" y="70" text-anchor="middle" class="label-text">Encoder $K$</text>
  <text x="240" y="175" text-anchor="middle" class="dim-text">$(B, n, d_k)$</text>
  
  <!-- Attention scores -->
  <rect x="340" y="60" width="140" height="120" class="attention" rx="3"/>
  <text x="410" y="50" text-anchor="middle" class="label-text">Attention Scores</text>
  <text x="410" y="195" text-anchor="middle" class="dim-text">$(B, m, n)$</text>
  <text x="410" y="210" text-anchor="middle" font-size="9" fill="#666" font-style="italic">$m \times n$ (not $m \times m$)</text>
  
  <!-- Arrows -->
  <path d="M 140 120 L 180 120" class="arrow"/>
  <path d="M 300 120 L 340 120" class="arrow"/>
  
  <!-- Encoder values -->
  <rect x="180" y="200" width="120" height="60" class="encoder" rx="3"/>
  <text x="240" y="190" text-anchor="middle" class="label-text">Encoder $V$</text>
  <text x="240" y="275" text-anchor="middle" class="dim-text">$(B, n, d_v)$</text>
  
  <!-- Output arrow -->
  <path d="M 410 180 L 410 200" class="arrow"/>
  <rect x="340" y="200" width="140" height="60" class="attention" rx="3"/>
  <text x="410" y="190" text-anchor="middle" class="label-text">Output</text>
  <text x="410" y="275" text-anchor="middle" class="dim-text">$(B, m, d_v)$</text>
</svg>
</div>

<p>Note that the attention score matrix is $(B, m, n)$, not $(B, m, m)$ or $(B, n, n)$, because we&rsquo;re attending from decoder positions to encoder positions.</p>

<h2 id="12-complete-dimension-flow-example">12. Complete Dimension Flow Example</h2>

<p>Let&rsquo;s trace through a concrete example with the original Transformer paper parameters:</p>

<ul>
<li>$B = 32$ (batch size)</li>
<li>$n = 100$ (source sequence length)</li>
<li>$V = 50000$ (vocabulary size)</li>
<li>$d_{\text{model}} = 512$</li>
<li>$h = 8$ (number of heads)</li>
<li>$d_k = d_v = 64$ ($512/8$)</li>
<li>$d_{\text{ff}} = 2048$</li>
</ul>

<p><strong>Dimension flow:</strong></p>

<div style="text-align: center; margin: 30px 0;">
<svg width="800" height="600" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <style>
      .box { fill: #f5f5f5; stroke: #666; stroke-width: 1.5; }
      .arrow { stroke: #666; stroke-width: 2; fill: none; marker-end: url(#arrowhead); }
      .label-text { font-family: 'Times New Roman', serif; font-size: 11px; font-weight: bold; }
      .dim-text { font-family: 'Times New Roman', serif; font-size: 9px; fill: #666; }
    </style>
    <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
      <polygon points="0 0, 10 3, 0 6" fill="#666"/>
    </marker>
  </defs>
  <!-- Input tokens -->
  <rect x="300" y="20" width="200" height="40" class="box" rx="3"/>
  <text x="400" y="45" text-anchor="middle" class="label-text">Input tokens</text>
  <text x="400" y="70" text-anchor="middle" class="dim-text">$(32, 100)$</text>
  
  <!-- Embeddings -->
  <path d="M 400 60 L 400 90" class="arrow"/>
  <rect x="300" y="90" width="200" height="40" class="box" rx="3"/>
  <text x="400" y="115" text-anchor="middle" class="label-text">Word embeddings</text>
  <text x="400" y="140" text-anchor="middle" class="dim-text">$(32, 100, 512)$</text>
  
  <!-- Positional encoding -->
  <path d="M 400 130 L 400 160" class="arrow"/>
  <rect x="300" y="160" width="200" height="40" class="box" rx="3"/>
  <text x="400" y="185" text-anchor="middle" class="label-text">+ Positional encoding</text>
  <text x="400" y="210" text-anchor="middle" class="dim-text">$(32, 100, 512)$</text>
  
  <!-- Multi-head attention -->
  <path d="M 400 200 L 400 230" class="arrow"/>
  <rect x="250" y="230" width="300" height="80" class="box" rx="3"/>
  <text x="400" y="250" text-anchor="middle" class="label-text">Multi-head attention</text>
  <text x="400" y="270" text-anchor="middle" class="dim-text">Q, K, V: $(32, 100, 64)$ → Scores: $(32, 100, 100)$</text>
  <text x="400" y="290" text-anchor="middle" class="dim-text">→ Concatenated: $(32, 100, 512)$ → Output: $(32, 100, 512)$</text>
  
  <!-- Residual + LayerNorm -->
  <path d="M 400 310 L 400 340" class="arrow"/>
  <rect x="300" y="340" width="200" height="40" class="box" rx="3"/>
  <text x="400" y="365" text-anchor="middle" class="label-text">Residual + LayerNorm</text>
  <text x="400" y="390" text-anchor="middle" class="dim-text">$(32, 100, 512)$</text>
  
  <!-- FFN -->
  <path d="M 400 380 L 400 410" class="arrow"/>
  <rect x="250" y="410" width="300" height="60" class="box" rx="3"/>
  <text x="400" y="430" text-anchor="middle" class="label-text">Feed-forward network</text>
  <text x="400" y="450" text-anchor="middle" class="dim-text">Expand: $(32, 100, 2048)$ → Contract: $(32, 100, 512)$</text>
  
  <!-- Residual + LayerNorm 2 -->
  <path d="M 400 470 L 400 500" class="arrow"/>
  <rect x="300" y="500" width="200" height="40" class="box" rx="3"/>
  <text x="400" y="525" text-anchor="middle" class="label-text">Residual + LayerNorm</text>
  <text x="400" y="550" text-anchor="middle" class="dim-text">$(32, 100, 512)$</text>
  
  <!-- Output projection -->
  <path d="M 400 540 L 400 570" class="arrow"/>
  <rect x="200" y="570" width="400" height="30" class="box" rx="3"/>
  <text x="400" y="590" text-anchor="middle" class="label-text">Output projection + Softmax</text>
  <text x="400" y="605" text-anchor="middle" class="dim-text">$(32, 100, 50000)$</text>
</svg>
</div>

<ol>
<li>Input tokens: $(32, 100)$</li>
<li>Word embeddings: $(32, 100, 512)$</li>
<li>+ Positional encoding: $(32, 100, 512)$</li>
<li>Multi-head attention:
   <ul>
   <li>Q, K, V projections: $(32, 100, 64)$ each</li>
   <li>Attention scores: $(32, 100, 100)$</li>
   <li>Attention weights: $(32, 100, 100)$</li>
   <li>Per-head output: $(32, 100, 64)$</li>
   <li>Concatenated: $(32, 100, 512)$</li>
   <li>Output projection: $(32, 100, 512)$</li>
   </ul>
</li>
<li>Residual + LayerNorm: $(32, 100, 512)$</li>
<li>Feed-forward network:
   <ul>
   <li>First linear: $(32, 100, 2048)$</li>
   <li>ReLU: $(32, 100, 2048)$</li>
   <li>Second linear: $(32, 100, 512)$</li>
   </ul>
</li>
<li>Residual + LayerNorm: $(32, 100, 512)$</li>
<li>Output projection: $(32, 100, 50000)$</li>
<li>Softmax: $(32, 100, 50000)$</li>
</ol>

<h2 id="13-key-insights">13. Key Insights</h2>

<ul>
<li><strong>Dimension preservation:</strong> The main data flow maintains $(B, n, d_{\text{model}})$ dimensions through most of the network, with temporary expansions in attention (to $(B, n, n)$ for scores) and FFN (to $(B, n, d_{\text{ff}})$).</li>

<li><strong>Attention complexity:</strong> The attention score matrix is quadratic in sequence length $(B, n, n)$, which is why Transformers have $O(n^2)$ complexity.</li>

<li><strong>Multi-head dimension math:</strong> With $h$ heads, each head uses $d_k = d_v = \frac{d_{\text{model}}}{h}$ dimensions. After concatenation, we get back to $d_{\text{model}}$ dimensions: $h \times \frac{d_{\text{model}}}{h} = d_{\text{model}}$.</li>

<li><strong>FFN expansion:</strong> The feed-forward network expands to $d_{\text{ff}}$ (typically $4 \times d_{\text{model}}$) before projecting back, allowing for richer representations.</li>
</ul>

<h2 id="14-conclusion">14. Conclusion</h2>

<p>Understanding matrix dimensions is essential for debugging Transformer implementations and ensuring correct tensor operations. The key pattern is that most operations preserve the sequence and model dimensions $(B, n, d_{\text{model}})$, with specific expansions for attention computations and feed-forward transformations. Keeping track of these dimensions at each step helps prevent shape mismatches and provides insight into the computational flow of the architecture.</p>

        
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;text=Transformer%20Attention%20Matrix%20Dimensions&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;title=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fjohnchennewyork-coder.github.io%2fblog%2fposts%2ftransformer-attention-dimensions%2f&amp;description=Transformer%20Attention%20Matrix%20Dimensions" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://johnchennewyork-coder.github.io/blog/posts/transformers_explained/" data-toggle="tooltip" data-placement="top" title="Transformers_explained">&larr; Previous Post</a>
            </li>
          
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              John Chen
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://johnchennewyork-coder.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.59.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://johnchennewyork-coder.github.io/blog/js/main.js"></script>
<script src="https://johnchennewyork-coder.github.io/blog/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body, {
  delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\[", right: "\\]", display: true},
    {left: "\\(", right: "\\)", display: false}
  ],
  throwOnError: false
}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://johnchennewyork-coder.github.io/blog/js/load-photoswipe.js"></script>





    
  </body>
</html>

