<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>todo on John&#39;s Machine Learning and Deep Learning Blog</title>
    <link>https://johnchennewyork-coder.github.io/blog/tags/todo/</link>
    <description>Recent content in todo on John&#39;s Machine Learning and Deep Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 27 Oct 2019 21:31:36 -0400</lastBuildDate>
    
	<atom:link href="https://johnchennewyork-coder.github.io/blog/tags/todo/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Auto Encoder</title>
      <link>https://johnchennewyork-coder.github.io/blog/posts/vae/</link>
      <pubDate>Sun, 27 Oct 2019 21:31:36 -0400</pubDate>
      
      <guid>https://johnchennewyork-coder.github.io/blog/posts/vae/</guid>
      <description>VAEs are autoencoders with some added randomness. The encoder network (also called recognition or inference network) outputs parameters of a probability distribution for each data point. Then, for each data point, we sample from this parametrized distribution, and feed the SAMPLE to the decoder network, which then is tasked with reconstructing the output. The training objective involves changing from an integral, into just doing optimization, hence the VAE variational objective. In particular, all we do is maximize the ELBO, which helps push up our p(x) which normally would require an integral!</description>
    </item>
    
  </channel>
</rss>