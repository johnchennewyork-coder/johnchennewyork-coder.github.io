<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Transformers_explained - John&#39;s Machine Learning and Deep Learning Blog</title>
  <meta name="description" content="Transformers are one of the crucial building blocks of modern day NLP. There has been much confusion on them, even though thousands of blog posts, animations exist. I finally clarified my understanding after a short 5 minute conversation!
So here is my brief, opinionated explanation of a transformer!
Transfomer:
 Is a parallelizable, and efficient way of performing neural language modelling Dispenses with RNNs; uses only attention Is non-autoregressive, and autoregressive  Because of those last two points, the architecture deals with inputs in a very special way!">
  <meta name="author" content="John Chen"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "John\x27s Machine Learning and Deep Learning Blog",
    
    "url": "https:\/\/johntiger1.github.io\/blog"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/johntiger1.github.io\/blog"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/johntiger1.github.io\/blog",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/johntiger1.github.io\/blog\/posts\/transformers_explained\/",
          "name": "Transformers explained"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "John Chen"
  },
  "headline": "Transformers_explained",
  "description" : "Transformers are one of the crucial building blocks of modern day NLP. There has been much confusion on them, even though thousands of blog posts, animations exist. I finally clarified my understanding after a short 5 minute conversation!\nSo here is my brief, opinionated explanation of a transformer!\nTransfomer:\n Is a parallelizable, and efficient way of performing neural language modelling Dispenses with RNNs; uses only attention Is non-autoregressive, and autoregressive  Because of those last two points, the architecture deals with inputs in a very special way!",
  "inLanguage" : "en",
  "wordCount":  609 ,
  "datePublished" : "2019-11-28T09:54:25",
  "dateModified" : "2019-11-28T09:54:25",
  "image" : "https:\/\/johntiger1.github.io\/blog\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/johntiger1.github.io\/blog\/posts\/transformers_explained\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/johntiger1.github.io\/blog",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/johntiger1.github.io\/blog\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Transformers_explained" />
<meta property="og:description" content="Transformers are one of the crucial building blocks of modern day NLP. There has been much confusion on them, even though thousands of blog posts, animations exist. I finally clarified my understanding after a short 5 minute conversation!
So here is my brief, opinionated explanation of a transformer!
Transfomer:
 Is a parallelizable, and efficient way of performing neural language modelling Dispenses with RNNs; uses only attention Is non-autoregressive, and autoregressive  Because of those last two points, the architecture deals with inputs in a very special way!">
<meta property="og:image" content="https://johntiger1.github.io/blog/img/avatar-icon.png" />
<meta property="og:url" content="https://johntiger1.github.io/blog/posts/transformers_explained/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="John&#39;s Machine Learning and Deep Learning Blog" />

  <meta name="twitter:title" content="Transformers_explained" />
  <meta name="twitter:description" content="Transformers are one of the crucial building blocks of modern day NLP. There has been much confusion on them, even though thousands of blog posts, animations exist. I finally clarified my â€¦">
  <meta name="twitter:image" content="https://johntiger1.github.io/blog/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary" />
  <link href='https://johntiger1.github.io/blog/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.59.0" />
  <link rel="alternate" href="https://johntiger1.github.io/blog/index.xml" type="application/rss+xml" title="John&#39;s Machine Learning and Deep Learning Blog"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://johntiger1.github.io/blog/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://johntiger1.github.io/blog/css/highlight.min.css" /><link rel="stylesheet" href="https://johntiger1.github.io/blog/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-150888192-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://johntiger1.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/blog">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/blog/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/blog/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="John&#39;s Machine Learning and Deep Learning Blog" href="https://johntiger1.github.io/blog">
            <img class="avatar-img" src="https://johntiger1.github.io/blog/img/avatar-icon.png" alt="John&#39;s Machine Learning and Deep Learning Blog" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="posts-heading">
              
                <h1>Transformers_explained</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>Transformers are one of the crucial building blocks of modern day NLP. There has been much confusion on them, even though thousands of blog posts, animations exist. I finally clarified my understanding after a short 5 minute conversation!</p>

<p>So here is my brief, opinionated explanation of a transformer!</p>

<p>Transfomer:</p>

<ul>
<li>Is a parallelizable, and efficient way of performing neural language modelling</li>
<li>Dispenses with RNNs; uses only attention</li>
<li>Is non-autoregressive, and autoregressive</li>
</ul>

<p>Because of those last two points, the architecture deals with inputs in a very special way!</p>

<p>Input: an entire sequence at once. Unlike an RNN, tokens are NOT fed one-by-one! For this reason, we do require a positional encoding!</p>

<p>Output: A single token. The transformer will look at the entire input sequence, as well as the previous words that it decoded/generated. <strong>This means that it is non-autoregressive in the input sequence; it can operate on the entire sequence at once. Hence why it requires an entire input sequence</strong></p>

<p>Some notes: During training, it can be trained in a very rapid way. Since most models are generally trained using teacher forcing anyways, we actually do not care about what tokens the transformer previously generated. Hence, during training time, we are essentially just maximizing the probability of producing a specific token (the next one, at time step t) conditioned on the input sequence, and the <em>groud truth</em> decoded output sequence 0 to t-1 , which are provided. This is where the causal triangular masking comes into play; where we hide the output from next step of the transformer; this is so we can train quickly (in a batched fashion; without waiting for the transformer to decode itself). Note this is also why we have an output positional embedding! This is so the transformer is able to correctly make sense of the ordering of its tokens.</p>

<p>However, during testing/inference/generation, it must be trained in a slow, autoregressive fashion.
This is because here, we do not have access to the ground truth tokens! In order to generate N tokens, we must run N forward passes (of the decoder; we can cache the model when run on the input sequence). This is similar to how a regular seq2seq model generates, as it moves the sliding window left to right. (kind of; depending on the architecture, a seq2seq can also &lsquo;just&rsquo; require N passes of the decoder)</p>

<p>We have all the regular generation details when running the forward pass of the decoder: it is a function $d(x<em>t, h</em>{t-1})$, which takes in the previous token just generated, and the previous hidden state(s) and then generates the output probabilities for the next token. Then, we run some max or other discrete to get the actual token generated at that timestep, and then we feed it in to the decoder at the next time step (probably going through a positional encoding as well)</p>

<p>Commentary:
Humans do not necessarily read exactly left to right. There is evidence that we do some type of convolution/high level scan of the words, at a high level, and then we go into more detail/precision (character by character) when we need. This could be another research idea too! (Hierarchical NLP)</p>

<p>Deep fake text generation has the potential to change the world in the future. We can solve this problem, via occupying the gap!</p>

<p>Recall the steps to writing the proposal
Overall, yesterday I learnt a lot of stuff! Went to the RL talk (on hierarchical reinforcement learning; where we have multiple agents each responsible for a single task; constrained state space, but shared action space), and then also went to the ARIA, where had great discussions with Pricing Optimization, Transformer, and the RL project.</p>


        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f&amp;text=Transformers_explained&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f&amp;title=Transformers_explained" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f&amp;title=Transformers_explained" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f&amp;title=Transformers_explained" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fjohntiger1.github.io%2fblog%2fposts%2ftransformers_explained%2f&amp;description=Transformers_explained" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://johntiger1.github.io/blog/posts/reproduction-headaches/" data-toggle="tooltip" data-placement="top" title="Reproduction Headaches">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://johntiger1.github.io/blog/posts/conflict-resolution/" data-toggle="tooltip" data-placement="top" title="Conflict Resolution">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      
        
        
      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          <li>
            <a href="" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              John Chen
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2020
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://johntiger1.github.io/blog">John&#39;s Machine Learning and Deep Learning Blog</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.59.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://johntiger1.github.io/blog/js/main.js"></script>
<script src="https://johntiger1.github.io/blog/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://johntiger1.github.io/blog/js/load-photoswipe.js"></script>









    
  </body>
</html>

