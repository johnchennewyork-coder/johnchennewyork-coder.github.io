<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on John&#39;s Machine Learning and Deep Learning Blog</title>
    <link>https://johntiger1.github.io/blog/posts/</link>
    <description>Recent content in Posts on John&#39;s Machine Learning and Deep Learning Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Nov 2019 19:04:31 -0500</lastBuildDate>
    
	<atom:link href="https://johntiger1.github.io/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hugo New Considered Harmful</title>
      <link>https://johntiger1.github.io/blog/posts/hugo-new-considered-harmful/</link>
      <pubDate>Sun, 10 Nov 2019 19:04:31 -0500</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/hugo-new-considered-harmful/</guid>
      <description>I&amp;rsquo;m trying to like Hugo. I really am.
But more and more it seems to me like the only reason this static site generator exists is because, &amp;ldquo;hey, why not?&amp;rdquo;. In other words, I am finding that the overhead of using the SSG is quite high, and interfering with my ability to do what I want, leading to me wondering why I am using it at all.
(For reference: as someone coming from a CS background, with some web dev experience, I could trivially make a simple and functional blog by myself, so the value prop for Hugo really seems limited)</description>
    </item>
    
    <item>
      <title>Topics in Deep Learning</title>
      <link>https://johntiger1.github.io/blog/posts/topics-in-cs/</link>
      <pubDate>Sat, 09 Nov 2019 00:38:18 -0500</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/topics-in-cs/</guid>
      <description>There are some new topics and muses for my study:
 Reinforcement learning (off-policy, on-policy) Embeddings; just generally getting accustomed to the lingo!  ex. a &amp;ldquo;question&amp;rdquo; embedding, or some other use of the word &amp;ldquo;embedding&amp;rdquo;  Graph neural networks Really want to concretize MCTS + Neural networks; i.e. the Alpha Zero framework  </description>
    </item>
    
    <item>
      <title>C3</title>
      <link>https://johntiger1.github.io/blog/posts/c3.ai/</link>
      <pubDate>Fri, 08 Nov 2019 01:25:25 -0500</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/c3.ai/</guid>
      <description>So I had my 1st round final interviews for c3.ai today! It was an OK experience.
The interview consisted of 3 portions, and I thought it was a pretty good interview process!
First interview: Let&amp;rsquo;s say you are working for Expedia. How would you build a model to predict which trips are being taken for business and which are being taken for leisure?
Second interview: Questions on domain knowledge of ML.</description>
    </item>
    
    <item>
      <title>Microsoft Onsite</title>
      <link>https://johntiger1.github.io/blog/posts/microsoft-onsite/</link>
      <pubDate>Tue, 05 Nov 2019 06:39:02 -0500</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/microsoft-onsite/</guid>
      <description>I&amp;rsquo;m currently sitting in the airport in Philadelphia, having just taken the 10:30 red-eye from Seattle. I had the entire row to myself, which enabled me to sleep rather decently, all things considered! (even better than I&amp;rsquo;m sleeping at 95 Lippincott in all honesty!)
My onsites were earlier that day in Redmond, Washington (Microsoft Main Campus). It was quite a process interviewing with them!
Here&amp;rsquo;s a breakdown of the entire day:</description>
    </item>
    
    <item>
      <title>Learning to Program</title>
      <link>https://johntiger1.github.io/blog/posts/learning-to-program/</link>
      <pubDate>Fri, 01 Nov 2019 18:40:45 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/learning-to-program/</guid>
      <description>This week, we covered a wide variety of approaches to neural methods to program synthesis and program induction.
The first batch of presentations: Romila, Eric and Henri Romila: we have synthesis; the most naive way is indeed one way that CAN be used to form programs: get the input space, the program space, the action space, and then try all possible combinations. Her paper talked about some sort of Monte Carlo tree search based method; alpha zero but applied to program synthesis, and she was critical as they don&amp;rsquo;t update the policy network every time.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://johntiger1.github.io/blog/posts/reinforcement-learning/</link>
      <pubDate>Thu, 31 Oct 2019 23:44:55 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/reinforcement-learning/</guid>
      <description>Today marks my first foray into reinforcement learning territory!
Although I have studied the topic extensively academically (well, kind of) I have not had a chance to actually sit down and play around with some implementations. So without further ado, here goes!
Observation #1: past classifications affect future performance.
A regular ol&amp;rsquo; CNN classifier is one-shot: it makes the prediction and then that&amp;rsquo;s it. Same with a RNN, sequential though it may be.</description>
    </item>
    
    <item>
      <title>Lyft_final_interview</title>
      <link>https://johntiger1.github.io/blog/posts/lyft_final_interview/</link>
      <pubDate>Thu, 31 Oct 2019 21:36:28 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/lyft_final_interview/</guid>
      <description>Wow! That was quite an intense and exhausting 3 hours!
I just finished my (virtual) final round interviews for Lyft.
I had 3 back-to-back interviews; the first one on coding, the second on experience and team fit (with manager) and then the third on ML domain knowledge. What did not help was the huge time difference: I started my interviews at 5:30 PM, and it went all the way to 8:30 PM Toronto time.</description>
    </item>
    
    <item>
      <title>Hugo vs Jekyll</title>
      <link>https://johntiger1.github.io/blog/posts/hugo-vs-jekyll/</link>
      <pubDate>Thu, 31 Oct 2019 11:36:57 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/hugo-vs-jekyll/</guid>
      <description>Seems like Jekyll wins! https://github.blog/2016-05-10-better-discoverability-for-github-pages-sites/</description>
    </item>
    
    <item>
      <title>Cross Aligned Autoencoders</title>
      <link>https://johntiger1.github.io/blog/posts/cross-aligned-autoencoders/</link>
      <pubDate>Thu, 31 Oct 2019 11:36:52 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/cross-aligned-autoencoders/</guid>
      <description>How does Shen&amp;rsquo;s paper work? Well it tries to fit a cross-aligned autoencoder. This is simply an autoencoder that is also guided adversarially by a discriminator. In terms of the basics/nitty gritty technical details, we have the following:
 We have a sequential autoencoder. This is &amp;ldquo;simply&amp;rdquo; a recurrent autoencoder: i.e. an autoencoder where the encoder accepts a sequence of states, and the decoder also generates a sequence of states. This sequential autoencoder is aligned by an adversary.</description>
    </item>
    
    <item>
      <title>Ordered Dict</title>
      <link>https://johntiger1.github.io/blog/posts/ordered-dict/</link>
      <pubDate>Wed, 30 Oct 2019 19:42:04 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/ordered-dict/</guid>
      <description>An OrderedDict is a dictionary that remembers insertion order. Sweet and simple! when comparing OrderedDicts, Python will check the order of the elements, as the name suggests!
from collections import OrderedDict od = OrderedDict() od[&amp;#34;a&amp;#34;] = 99 od[&amp;#34;b&amp;#34;] = 100 od.pop() #pops the most recently entered entry! ... It also has some minor methods relating to this order:
od.move_to_end(&amp;#39;b&amp;#39;, last=True) # moves key b to end of ordering od.move_to_end(&amp;#39;b&amp;#39;, last=False) # moves key b to beginning of ordering</description>
    </item>
    
    <item>
      <title>Data Science and Machine Learning Review</title>
      <link>https://johntiger1.github.io/blog/posts/datascience-review/</link>
      <pubDate>Wed, 30 Oct 2019 19:38:53 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/datascience-review/</guid>
      <description>As I prepare for my final Machine Learning interview with Lyft, I look back on my Data Science career and projects. I also want to give a brief overview of the subjects that are useful for machine learning and data science.
Data science type positions: 1. Basic statistics 2. Basic probability 3. Linear Regression 4. Normal equation
It is difficult to draw the line precisely between machine learning and data science.</description>
    </item>
    
    <item>
      <title>Deque</title>
      <link>https://johntiger1.github.io/blog/posts/deque/</link>
      <pubDate>Wed, 30 Oct 2019 19:38:47 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/deque/</guid>
      <description>A deque (pronounced &amp;ldquo;deck&amp;rdquo;) is a doubly-ended queue data structure in Python. It is just like your regular array, but it has faster append/pop operations!
from collections import deque d = deque() d.appendleft(2) d.popleft() d.pop() d.append() It can also support rotation, and other nice features!</description>
    </item>
    
    <item>
      <title>Interview Tips</title>
      <link>https://johntiger1.github.io/blog/posts/interview-tips/</link>
      <pubDate>Tue, 29 Oct 2019 23:21:23 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/interview-tips/</guid>
      <description>As a follow-up to my recent interview-recap on G-Research, I&amp;rsquo;ve come to some realizations on some general tips for doing well on technical interviews:
 Don&amp;rsquo;t procrastinate!  Scheduling a coding challenge might feel like scheduling a dentist appointment: why are you willingly scheduling pain for yourself?! But like scheduling a dentist appointment, it is a necessary fact of life. Schedule it soon as you have a relatively free day (I know, I know everyone is always busy!</description>
    </item>
    
    <item>
      <title>Whats in a Name Review</title>
      <link>https://johntiger1.github.io/blog/posts/whats-in-a-name-review/</link>
      <pubDate>Tue, 29 Oct 2019 13:21:25 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/whats-in-a-name-review/</guid>
      <description>Paper review: (Whatâ€™s in a Name? Reducing Bias in Bios Without Access to Protected Attributes)[https://arxiv.org/abs/1904.05233]
Overall, I have a negative opinion of this paper. It introduces a novel way of bias mitigation, but provides little theoretical justification for why such an approach is useful, and also does not compare their work with any previous work. More over, I am extremely skeptical of the entire point of their paper, which is that we do not have access to protected attributes and so should find a way of providing fairness without looking at protected attributes.</description>
    </item>
    
    <item>
      <title>On Papers</title>
      <link>https://johntiger1.github.io/blog/posts/on-papers/</link>
      <pubDate>Tue, 29 Oct 2019 12:01:33 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/on-papers/</guid>
      <description>As a follow-up to my recent post on research tips, I would like to mention that confidence and story-telling is key. Many papers have some flaws, and substantial overlap with existing papers, which becomes especially apparent once you get to know the field. The key is presenting a convincing narative that steers the discussion away from focusing too much on those flaws and similarity. In particular, when you are building the introduction, you want to build and hammer home a singular line of thought, a single progression of thought, which makes it OBVIOUS that YOUR approach is the most logical next step forward.</description>
    </item>
    
    <item>
      <title>Pure Storage Coding Challenge</title>
      <link>https://johntiger1.github.io/blog/posts/pure-storage-coding-challenge/</link>
      <pubDate>Tue, 29 Oct 2019 00:27:14 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/pure-storage-coding-challenge/</guid>
      <description>Just finished the Pure Storage Coding Challenge! It consists of some coding questions, and then some CS theory questions. Overall, it was a very fair coding challenge, and actually had interesting questions that test your knowledge of CS beyond just Leetcode :)
Great test!</description>
    </item>
    
    <item>
      <title>Research Tips</title>
      <link>https://johntiger1.github.io/blog/posts/research-tips/</link>
      <pubDate>Mon, 28 Oct 2019 00:36:02 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/research-tips/</guid>
      <description>Research is hard. Here are some tips I&amp;rsquo;ve picked up over the years: 1. Try to present your work to as many people who will listen. It forces you to really understand the content yourself, and serves as a reality check 2. Make dedicated time for research. As a grad student, it is easy to put research to the side, but to solve any non-trivial problem requires constant, structured attention. 3.</description>
    </item>
    
    <item>
      <title>Variational Auto Encoder</title>
      <link>https://johntiger1.github.io/blog/posts/vae/</link>
      <pubDate>Sun, 27 Oct 2019 21:31:36 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/vae/</guid>
      <description>VAEs are autoencoders with some added randomness. The encoder network (also called recognition or inference network) outputs parameters of a probability distribution for each data point. Then, for each data point, we sample from this parametrized distribution, and feed the SAMPLE to the decoder network, which then is tasked with reconstructing the output. The training objective involves changing from an integral, into just doing optimization, hence the VAE variational objective. In particular, all we do is maximize the ELBO, which helps push up our p(x) which normally would require an integral!</description>
    </item>
    
    <item>
      <title>Upcoming</title>
      <link>https://johntiger1.github.io/blog/posts/upcoming/</link>
      <pubDate>Sun, 27 Oct 2019 17:43:51 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/upcoming/</guid>
      <description>So without further ado, here are the posts I plan to make:
 Exotic python data structures and where to find them (heapq, OrderedDict, Deque, bisect) Onsite interview with Microsoft Reparameterization tricks: Gaussian vs gumbel The REINFORCE gradient estimator Probability notations Research itinerary Birthday post ABD, L1, and Deep learning Symposium follow-up Scale AI follow-up The supremum IS big-O notation (usually we want the TIGHTEST worst case bound) Model vs Data Parallelism [Some more reinforcement learning][1] [Some more reinforcement learning]1  (Note that the markdown in preview does not exactly work with some more exotic features, like citations and references)</description>
    </item>
    
    <item>
      <title>Roadmap</title>
      <link>https://johntiger1.github.io/blog/posts/roadmap/</link>
      <pubDate>Sun, 27 Oct 2019 15:32:39 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/roadmap/</guid>
      <description>What exactly is the value prop of this blog? Well I like to think that I have some interesting things going on in my life:
 Machine learning, deep learning, linear algebra, statistics. I am a student in deep learning, and am lucky enough to be exposed daily to the inspirational and amazing work being conducted in the U of T ML group, at Vector Institute. In particular, I want to talk about the awesome papers and concepts I learn about in CSC2547 and CSC2541 My research: to appear in ACL 2020 (fingers crossed!</description>
    </item>
    
    <item>
      <title>How to make a Hugo Subsite</title>
      <link>https://johntiger1.github.io/blog/posts/hugo-subsite/</link>
      <pubDate>Sat, 26 Oct 2019 23:18:32 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/hugo-subsite/</guid>
      <description>You may have noticed that this blog exists under my main Github personal website. How did I get it to work?
 Github exposes your entire vitual directory by default. (This is why you can just put raw assets in the repo, and they will be served when queried directly) So, I just made Hugo build the entire blog subsite to a folder in my main repo!
  In short:</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>https://johntiger1.github.io/blog/posts/first-public-post/</link>
      <pubDate>Sat, 26 Oct 2019 18:55:39 -0400</pubDate>
      
      <guid>https://johntiger1.github.io/blog/posts/first-public-post/</guid>
      <description>I&amp;rsquo;ve finally set up the Hugo blog subsite on my personal github site, so here is the obligatory &amp;ldquo;Hello, world!&amp;rdquo;
Current thoughts about Hugo: Neat, but probably would have been better served by a more established/feature-rich system like Jekyll or perhaps Gatsby!
In particular: I have concerns about dating/naming of posts, especially as this list grows (hopefully) long!</description>
    </item>
    
  </channel>
</rss>